# Разделяй и властвуй

Общая идея - для того что бы решить задач - разбираем задачу на несколько более простых
подзадач. Далее решаем эти подзадачи рекурсивно. И из полученных ответов для подзадач строится
ответ для исходной подзадачи.

## Поиск в неупорядоченном массиве

Естественный алгоритм - линейный поиск, который сканирует 1 раз массив. Сложность O(n). Любой
алгоритм, который не прочитал весь массив не может с уверенностью сказать, что ключа k в массиве
нет.

## Поиск в упорядоченном массиве

Двоичный поиск. На каждом шаге цель уменьшить область поиска в два раза. Если ищем в массиве
значение k, то первым делом сравниваем это значение со средним значением в массиве (тот, что в
середине массива). Если он меньше, то точно знаем, что k в левой части, и наоборот, если больше.
Вводим два индекса, который будут задавать текущий отрезок в массиве. Изначально l = 1, r = n.
Ищем средний элемент массива m = (l+r)/2 (floor) или l + (r-l)/2, что бы не было переполнения типов.
Если m совпадает с k, то возвращаем его, если меньше, то r = m - 1, иначе l = m + 1.    
Сложность - O(log n) на каждом шаге размер отрезка уменьшается вдвое.

## Умножение чисел

В столбик умножать (O(n)) - не оптимально.
Сложение в двоичной системе счисление. Идем с младшего бита к старшему (справа налево).    
При умножении в двоичной системе счисления так же идем справа налево. Надо умножить младший бит
(цифра) второго числа на все биты (цифры) другого числа. При этом каждый следующий этап
умножения - добавляет сдвиг налево. В итоге получим в столбик столько последовательностей битов,
сколько раз умножали цифру одного числа на все цифры другого со сдвигами налево. После чего
складываем результат. Всего будет n сложений. Максимальный сдвиг не больше 2n. Сложность
квадратична $O(n^2)$.

### Рекуррентная формула 1

- $y = 2 * (int) Math.floor(y/2)$ если y четное
- $y = 1 + 2 * (int) Math.floor(y/2)$ если y нечетное

Домножим все на x

- $x * y = 2 * (x * (int) Math.floor(y/2))$, если y четное
- $x * y = x + 2 * (x * (int) Math.floor(y/2))$, если y нечетное

y/2 округленное вниз - это число $y$ с отброшенным последним битом

Функция multiply    
Вход - два n-битовых целых числа $x>=0$ и $y>=0$. Где $n$ - это длина максимального из чисел. Будем
считать, что числа одинаковой длины, просто дописав достаточное количество нулей спереди.    
Выход - результат умножения двух чисел.

Если y = 0:    
&nbsp; вернуть 0    
$z = multiply(x, (int) \lfloor (y/2) \rfloor$  
если $y$ четно:    
&nbsp; вернуть $2z$    
иначе:    
&nbsp; вернуть $x + 2z$

Сложность - $O(n^2)$ - не более чем n рекурсивных вызовов, и в каждом вызове делается линейная работа.

### Рекуррентная формула 2

Данная формула позволит получить квадратичный алгоритм, который можно будет улучшить так, что бы
он работал асимптотически быстрее чем $O(n^2)$

Надо разбить числа на две части.

1) $x = x_L | x_R = 2^{(n/2)} * x_L + x_R$
2) $y = y_L | y_R = 2^{(n/2)} * y_L + y_R$
3) $x * y = (2^{(n/2)} * x_L + x_R) * (2^{(n/2)} * y_L + y_R) = 2^n * x_L * y_L + 2^{(n/2)} * (x_L *
   y_R + x_R * y_L) + x_R * y_R$

Сложность = $T(n) = 4T(n/2) + O(n)$, где 4T - это обработка четырех рекурсивных вызовов.

### Улучшенная рекуррентная формула 2

Вместо четырех рекурсивных вызовов для вычисления $x_L * y_L$, $x_L * y_R$, $x_R * y_L$, $x_R * y_R$
сделаем ти вычисления $x_L * y_L$, $x_R * y_R$ и $(x_L + x_R) * (y_L + y_R)$    
Тогда $(x_L * y_R + x_R * y_L) = (x_L + x_R) * (y_L + y_R) - x_L * y_L - x_R * y_R$   
Сложность - $T(n) = 3T(n/2) + O(n)$

#### Алгоритм Карацуба

**karatsuba(x, y)**    
Вход - целые числа $x,y >= 0$ в двоичной записи. $n$ максимальный размер чисел    
Выход - результат умножения $x*y$

$n = max(размер x, размер y)$  
если $n = 1$:  
&nbsp; вернуть $x*y$

$x_L, x_R$ = левые $\lceil{n/2}\rceil$ округлением вверх, правые $\lfloor{n/2}\rfloor$ округлением вниз битов x    
$y_L, y_R$ = левые $\lceil{n/2}\rceil$ округлением вверх, правые $\lfloor{n/2}\rfloor$ округлением вниз битов y

$P1 = karatsuba(x_L, y_L)$    
$P2 = karatsuba(x_R, y_R)$    
$P3 = karatsuba(x_L + x_R, y_L + y_R)$

вернуть $P1 * 2^{\lfloor{n/2}\rfloor округленную вниз} + (P3 - P1 - P2) * 2^{(\lfloor{n/2}\rfloor округленную вниз)} + P2$

Сложность - $O(n^{1.6})$. Есть еще быстрее алгоритмы основанные на быстром преобразовании Фурье.

## Произведение матриц

Считаем, что матрицы квадратные и одного размера
Умножение матриц - надо каждую строчку (i) одной матрицы домножить на каждый столбец (j) другой
матрицы, при этом должно получится одно значение, которое попадет в (i, j) элемент матрицы
произведения. Как этого достичь? Надо (i1 * j1) + (i2 * j2) + ...     
Сложность - O(n^3)

Даны две матрицы с размером n = 2 X и Y.
Данная формула верна и в случае, когда X и Y - это матрицы размера nxm, а Xij, Yij - это их
подматрицы размера n/2 x n/2. То есть по сути опять разбиваются матрицы на две 4 части и
происходит перемножение подматриц. Время работы T(n) = 8T(n/2) + O(n^2). То есть делаем 8
вызовов рекурсивных по умножению матриц.

**Алгоритм Штрассена**

Если XY = {

|                 |                 |
|-----------------|-----------------|
| X11Y11 + X12Y21 | X11Y12 + X12Y22 |
| X21Y11 + X22Y21 | X21Y12 + X22Y22 |

}

То применив алгоритм Штрассена получим:    
XY = {

|             |             |
|-------------|-------------|
| P5+P4-P2-P6 | P1+P2       | 
| P3+P4       | P1+P5-P3-P7 |

}

P1 = X11(Y21 - Y22)    
P2 = (X11 + X12)Y22    
P3 = (X21 + X22)Y11    
P4 = X22(Y21 - Y11)    
P5 = (X11 + X22)(Y11 + Y22)    
P6 = (X12 - X22)(Y21 + Y22)    
P7 = (X11 - X21)(Y11 + Y12)

Сложность - O(n^(log2_7)) = O(n^2.807)

Алгоритмы умножения матриц используются в алгоритмах на графах. Например, что бы узнать есть ли
треугольник (три вершины соединенные ребром). Стандартный способ это сделать - написать матрицу
смежности и возвести ее в куб.

## Сортировка слиянием

На вход дается массив A[1..n] . На выходе **перестановка** A'[1..n] элементов массива A[1..n], в
который элементы упорядочены по убыванию: $A'[1] <= A'[2] <= .. <= A'[n]$

Алгоритм имеет доступ к оракулу сравнения. Считаем, что сравнение занимает константное время.    
Если A = A', то алгоритм сортирует на месте, то есть не использует дополнительной памяти.

Оракул сравнения - внешняя процедура.

### Наивный алгоритм - сортировка вставками.

для i от 2 до n:    
j = i    
пока j>1 и A[j] < A[j-1]:    
обменять A[j] и A[j-1]    
j = j-1

То есть для каждого слудующего элемента в массиве проверяем, если эоемент больше предыдущего, то
ок, если меньше, то гоним этот элемент налево до тех пор пока текущий элемент (j) не будет больше
предыдущего (j-1). Таким образом все элементы от начала до j будут отсортированы. Массив
сортируется на месте.    
Сложность - O(n^2).

### Сортировка слиянием

mergesort(A, l, r)    
если l < r:    
m = (l+r)/2 округленный вниз    
merge(mergesort(A,l,m), mergesort(A, m+1, r))

Сортировка различных кусков массива рекурсивно. Алгоритм делает хоть, что-то только с массивом
размером больше двух. Есть массив, надо отсортировать в нем кусок от l до r, вычисляем индекс
среднего элемента, после этого делается отдельный рекурсивный вызов для куска слева и справа.
После этого надо сделать слияние результатов процедурой merge.   
Процедура merge:

* сливает два упорядоченных массива в один
* Работает за линейное от суммы размеров данных двух массивов: первым элементом
  результирующего массива будет меньший из первых элементов данных массивов, оставшаяся часть
  может быть заполнена рекурсивно. То есть два массива, смотрим на первый элемент этих двух
  массивов и берем минимальный из них и добавляем его в новый массив. **О добавленном элементе
  забываем. Таким образом можно всегда брать первый элемент в массивах и их сравнивать на каждой
  итерации** Повторяем до тех пор пока оба массива не пустые. Когда один массив стал пустым, а
  второй нет, то просто остатки дописываются в конец к результирующему массиву, так как массивы
  может быть разной длины.

Сложность - O(n log n)

**Итеративная сортировка слиянием:**        
функция intertivemergesort(A[1..n])    
Q = [] пустая очередь    
для i от 1 до n:    
pushback(Q, [A[i]])    
пока |Q| > 1:    
pushback(Q, merge(popfront(Q), popfront(Q)))    
вернуть popfront(Q)

_pushback(Q, [A[i]])_ - кладем все элементы в очередь в виде одноэлементных массивов    
_pushback(Q, merge(popfront(Q), popfront(Q)))_ - достаем из начала очереди два элемента, эти два
элемента сливаются, и после этого слитые два массива в виде массива побольше кладутся в конец
очереди. И так делает сначала для массивов с одним элементом, потом с двумя и так далее.

Лемма - любой корректный алгоритм сортировки, основанный на сравнениях элементов, делает Omega(n
log n ) сравнений в худшем случае на массив размера n.

**Заключение**

1) Наивный алгоритм сортировки имеет время работы O(n^2)
2) алгоритм сортировки слиянием, основанный на методе разделяй и властвуй, работает за время O(n
   log n). Использует O(n) дополнительной памяти для слияние массивов.
3) Любой алгоритм, сортирующий сравнениями, в худшем случае делает Omega(n log n) сравнений.
   Поэтому алгоритм сортировки слиянием асимптотически оптимален

## Быстрая сортировка

Рекурсивный алгоритм quicksort(A, l, r)
если l>=r:    
вернуть    
m = partition(A,l,r)    
quicksort(A,l,m-1)    
quicksort(A,m+1,r)

partition - переставляет элементы внутри массива от l до r так, что бы все элементы меньше x на
месте m были слева от x, и элементы больше x были справа от m. x в этой ситуации стоит на своем
месте. После этого достаточно отсортировать все что стоит до x и после x (двойной вызов
quicksort). В итоге важно, что бы после вызова процедуры элемент x был на своем месте, где m -
это его индекс.

Основные идеи partition:

1) x = A[l] - опорный элемент. Сначала берется первый элемент подмассива
2) двигаем i от l+1 до r, поддерживая следующий инвариант:

- A[k] <= x для всех l+1 <= k <= j
- A[k] > x для всех j+1 <= k <= i

3) Пусть i только что увеличился. Если A[i] > x, не делаем ничего, в противном случае меняем A[i]
   с A[j+1] и увеличиваем j.

partition(A,l,r):    
x = A[l]    
j = l    
для i от l+1 до r:    
____ если A[i] >= x:    
________ j=j+1    
_____ иначе обменять A[j] и A[i]    
обменять A[l] и A[j]    (после того, как весь массив был пройден ставим x на правильное место)    
вернуть j

Время работы O(n) = O(2 - l + 1) длина текущего подмассива

**Плохие и хорошие разделители**    
Время работы зависит от того насколько алгоритму повезло с разделителем (первый элемент
отсортированного куска). Если массив был отсортированный изначально, то алгоритм будет работать
за время O(n^2). Если нам повезло и массив всегда разбивается пополам, то время работы будет O(n
log n).    
Что бы разбить А относительно случайного разделителя, обменяем A[l] со случайным элементом и
вызовем partition(A,l,r)    
Допустим, что все элементы массива A[1...n] различны и что разделитель всегда выбирается
равномерно случайным образом. Тогда среднее время работы алгоритма quicksort(A) есть O(n log n),
в то время как время работы в худшем случае есть O(n^2). Усреднение берется по случайным числам
алгоритма, а не по входам.
Если все элементы массива равны между собой, то рассмотренная реализация алгоритма быстрой
сортировки будет работать квадратичное время. Что бы это обойти, массив можно разбивать на три
части вместо двух: <x , =x, >x

**Элиминация хвостовой рекурсии**    
Хвостовая рекурсия - это когда последнее что делает алгоритм - это вызывает самого себя.
Некоторые компиляторы умеют сами решать проблему хвостовой рекурсии.    
quicksort(A,l,r):    
пока l<r:    
m = partition(A,l,r)    
quicksort(A,l,m-1)   (левый рекурсивный вызов)     
l=m+1    (следующим шагом рекурсии сделает правый рекурсивный вызов)

Элиминируя рекурсивный выов для более длинного массива, мы гарантируем, что глубина рекурсии(а
значит и дополнительная память) будет в худшем случае не более O(log n). То есть после того, как
m посчитался надо выбрать из двух массивов короткий и вызвать quicksort для него, а вызов для
длинного массива элиминировать.

**Интроспективная сортировка**    
$O(n * log {_n})$ в худшем случае

- Запускает быструю сортировку с простой эвристикой выбора разделителя, например, медиана из
  первого, среднего и последнего элементов. Эвристика - простой метод исходя из разумных
  соображений.
- Если глубина рекурсии превышает порог c*logn, быстрая сортировка прерывается и запускается
  алгоритм с гарантированным временем O(n log n) в худшем случае, например сортировка кучей.

**Заключение**

- быстрая сортировка работает за время O(n log n) в среднем случае и за O(n^2) в худшем случае.
- Усреднение берется по случайным числам алгоритма, но не по входам
- Очень эффективен на практике
- Если в массиве могут быть равные числа, стоит использовать 3-разбиение вместо 2-разбиения.
- Элиминация хвостовой рекурсии позволяет сделать так, что бы алгоритм быстрой сортировки
  использовал не более O(log n) дополнительной памяти.

## Порядковые статистики

### Постановка задачи k-я порядковая статистика

Вход: массив $A[i...n]$
Выход: k-й элемент упорядоченный по неубыванию массива, то есть $A'[k]$. То есть какой элемент будет
стоять на k-м месте, если массив упорядочить по неубыванию.

1-я порядковая статистика - минимум в массиве    
k-я порядковая статистика - максимум    
n/2 порядковая статистика - медиана массива

Можно решить эту задачу так: просто упорядочить весь массив и вернуть k-й элемент. Но надо ли вообще
делать больше работы чем нужно?

Найти минимум или максимум, и это легко сделать за один проход. А вот как найти медиану - вопрос.

### Алгоритм (линейное время)

Функция $RandomSelect(A, l, r, k)$:  
&nbsp; если $l>=r$:  
&nbsp;&nbsp; выбрать случайный элемент x из $A[l...r]$  
&nbsp;&nbsp; разбить $A[l ... r]$  
&nbsp;&nbsp; на $A[l ... m{_1}]$, $A[m{_1}+1 ... m{_2}]$, $A[m{_2}+1 ... r]$  
&nbsp; если $l<=k<=m$:  
&nbsp;&nbsp; вернуть $RandomSelect(A, l, m{_1}, k)$  
&nbsp; иначе если $m{_1}+1 <= k <= m{_2}$:  
&nbsp;&nbsp; вернуть x  
&nbsp; иначе
&nbsp;&nbsp; вернуть $RandomSelect(A, m{_2}+1, r, k)$

A - массив  
l,r - индексы в которых мы ищем k  
k - искомый индекс

Так же как и в алгоритме быстрой сортировки мы выбираем случайный элемент относительно которого мы
разделяем массив на три части:

1) $A[l ... m{_1}]$ - все строго меньше x
2) $A[m{_1}+1 ... m{_2}]$ - равны x
3) $A[m{_2}+1 ... r]$ - строго больше x

Любая из этих частей может быть пустой, кроме средней, так как в нее x точно попадет.

Нам надо понять какой элемент будет стоять в индексе $k$ в этом массиве. Дальше мы просто смотрим в
какую из частей попал $k$, и для этой части делаем рекурсивный вызов и там пытаемся понять кто будет
стоять в ячейке с индексом $k$. Если $k$ попал в среднюю часть, то мы знаем точно, что на этом месте
обязан стоять $x$. То есть после того как произошло разбиение мы знаем в какой части массива
искать $k$.

#### Оценка времени работы алгоритма

##### Вспомогательная лемма

Математическое ожидание количества подбрасываний монетки до первой решки (включительно) равно 2.

###### Доказательство

$E = \frac{1}{2} * 1 + \frac{1}{2} * (1 + E)$ и следовательно, $E = 2$ или по
определению: $E = \sum_{i=1}^{\infty} i * \frac{1}{2^i} = 2$

Лемма

Среднее время работы алгоритма RandomSort есть O(n)

Доказательство

* Пусть $T(n)$ - среднее время работы
* $T(n) <= T(3n/4) + O(n)$:
    * время работы для массива размера n <= время работы для массива размером (3n/4) + время на
      уменьшение размера массива до <= 3n/4

Время уменьшения массива - мы выбираем разделитель каждый раз случайно. Массив делится на какие-то
части и мы выбираем какую часть и с ней продолжаем работу. Мы ждем пока эта часть на останется
размером <= 3n/4. то случайная величина. В среднем нам надо выбрать два элемента пока не будет
выбран хороший элемент. Это такой элемент, который уменьшит массив до <= 3n/4.

Значит, $T(n) = O(n)$ - бывающая геометрическая прогрессия.

##### Факт

Существует алгоритм, который может обработать любую порядковую статистику в худшем случае за время
O(n).






